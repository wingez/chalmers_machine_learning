{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group nr:\n",
    "\n",
    "Name 1 and CID: name surname (CID)\n",
    "\n",
    "Name 2 and CID: name surname (CID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.8.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from mining_world import Environment\n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining world "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/poster.png\" width=\"800\"/>\n",
    "\n",
    "## Scenario\n",
    "\n",
    "\n",
    "Humanity has now reached a point where we need to extract and refine more Copium, a precious resource with great value. The only problem is that Copium can only be found on certain uninhabitable planets. This of course means that automated robots are sent instead.      \n",
    "\n",
    "Copium is naturally very unstable and is only exists very temporary before it decays. There are very specific geological activities and circumstances needed for copium to form. The life cycle of Copium follows. First, a hot stream of liquid magma flows to the surface, creating a hotspot that that looks like a small creater. At the surface, if the conditions are correct, copium can form during the cool-down period. But as stated previously, Copium is unstable in its natural environment and decays to other materials shortly after. \n",
    "\n",
    "The formation of these deposits creaters are very random, but the heat from them can easilly be detected with a satellite. But there is no way of knowing if the newly formed depoist contains copium from just a satellite, therefor there is a robot rover on the ground with sensors that can collect further measurements. The rover's job is to move to the hotspots and identify if there could be Copium there or not. The rover has many sensors that can measure the properties of the ground below it, but of course, Copium can not directly be detected with these types of sensors. This is where the machine learning approach will be used, to take all those measurements and try to classify if the deposit contains Copium or not.  \n",
    "\n",
    "\n",
    "<img src=\"imgs/overView.png\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The enviornment\n",
    "\n",
    "The enviornment can be initilized like below. For each step a direction is specified (North, South, East, West) for the rover."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions \n",
    "\n",
    "<img src=\"imgs/actions.png\" width=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible actions ['N', 'S', 'W', 'E']\n"
     ]
    }
   ],
   "source": [
    "env = Environment(map_type=1, fps=5, resolution=(1000, 1000))\n",
    "actions = env.get_action_space()  \n",
    "print('Possible actions', actions)\n",
    "for i in range(20):\n",
    "    env.step(random.choice(actions) )# random action.\n",
    "    env.render() \n",
    "\n",
    "env.exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation - Tree search\n",
    "\n",
    "This section will show how the naivigation is done. This is not a part of the assignment to understand, but will be used. \n",
    "\n",
    "##  Breadth first\n",
    "\n",
    "The method used is a breadth first search algorithm, it is one of the simplest tree search algorithms and basically tries every option for a fixed number of steps and chooses the best one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, actor):\n",
    "        self.actor = actor\n",
    "        self.total_score = 0\n",
    "    \n",
    "    def update(self, action, inherited_score):\n",
    "        score = self.actor.step(action)\n",
    "        self.total_score = 1.05*inherited_score + score\n",
    "        return self.total_score\n",
    "    \n",
    "    def get_score(self):\n",
    "        return self.total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breadth_first_search(actor, max_depth, action_space):\n",
    "    node = Node(copy.deepcopy(actor)) \n",
    "    queue_keys = ['0'] # queue to keep track of nodes that has not yet been expanded.  \n",
    "    visited = {queue_keys[0]: node} # saves visited nodes in order to not recalulate the entire path for each step. \n",
    "    \n",
    "    max_score = -np.inf\n",
    "    best_action = None\n",
    "\n",
    "    while True:\n",
    "        key = queue_keys.pop(0)\n",
    "        if len(key) > max_depth: # stop at a set depth \n",
    "            break    \n",
    "        node = visited[key]\n",
    "        \n",
    "        for action in action_space: # expand all children nodes\n",
    "            child_node = copy.deepcopy(node)  # copy current node\n",
    "            score = child_node.update(action=action, inherited_score=node.get_score()) # update node with action\n",
    "            child_key = key + action # create child node key\n",
    "            \n",
    "            if score > max_score: # save best path \n",
    "                max_score = score\n",
    "                best_action = child_key[1]\n",
    "                \n",
    "            visited[child_key] = child_node  # add child node to visited nodes.\n",
    "            queue_keys.append(child_key)  # add child node queue of non expanded nodes. \n",
    "            \n",
    "    return best_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(map_type=1, fps=10, resolution=(1000, 1000))\n",
    "\n",
    "for i in range(100):\n",
    "    action = breadth_first_search(actor=env.get_actor(), max_depth=3, action_space=env.get_action_space())\n",
    "    env.step(action)\n",
    "    env.render()\n",
    "\n",
    "env.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exersice 1: Collect data\n",
    "The first step is to collect some data that will be used for training and validation. The available types features can be seen with env.get_sensor_properties() and the actual measurements can be retrieved with env.get_sensor_readings(). It will return a dictionary with the same keys as in env.get_sensor_properties() containg a value for each feature. If the robot is not currently over a deposit, then it will return None. The label can be extracted with env.get_ground_truth(), which will return a 1 if there is copium in the deposit and 0 if not. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor properties ['ground_density', 'moist', 'reflectivity', 'silicon_rate', 'oxygen_rate', 'iron_rate', 'aluminium_rate', 'magnesium_rate', 'undetectable']\n"
     ]
    }
   ],
   "source": [
    "sensor_properties = env.get_sensor_properties()\n",
    "print('Sensor properties', sensor_properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exersice is then to append the sensor readings to the cooresponing feature in the data dictionary and if there is copium or not.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(map_type=1, fps=500, resolution=(1000, 1000))\n",
    "sensor_properties = env.get_sensor_properties()\n",
    "\n",
    "# We can initilize the dictionary the following way.\n",
    "data = dict()\n",
    "data['copium'] = [] \n",
    "for key in sensor_properties:\n",
    "    data[key] = []\n",
    "    \n",
    "for i in range(5000):\n",
    "    action = breadth_first_search(actor=env.get_actor(), max_depth=3, action_space=env.get_action_space())\n",
    "    env.step(action)\n",
    "    # if we are over a deposit. \n",
    "    if env.get_sensor_readings() is not None:\n",
    "        sensor_readings = env.get_sensor_readings()\n",
    "        copium = env.get_ground_truth()\n",
    "        \n",
    "        # TODO: Append the sensor readings and copium to the data dictionary\n",
    "        for s in sensor_readings:\n",
    "            data[s].append(sensor_readings[s])\n",
    "        data['copium'].append(copium)\n",
    "        \n",
    "        \n",
    "    #env.render()\n",
    "\n",
    "env.exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exerscie 2: Data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Pandas data frame \n",
    "In this assignment we will work pandas data frame for storing the collected data. First create a pandas data frame from the dictionary. The documentation for it can be found at https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html, only the data feild needs to be filled in with the created dictionary. Call this data frame df. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      copium  ground_density     moist  reflectivity  silicon_rate  \\\n",
      "0          0        0.491701  0.219667      0.279961      0.201876   \n",
      "1          0        2.019307  0.208632      0.450697      0.072856   \n",
      "2          0        1.769909  0.153068      0.040787      0.194635   \n",
      "3          0        1.660560  0.207800      0.378206      0.043235   \n",
      "4          0        0.974718  0.233703      0.180920      0.066419   \n",
      "...      ...             ...       ...           ...           ...   \n",
      "1574       0        1.200625  0.178378      0.125998      0.071152   \n",
      "1575       0        1.234500  0.048722      0.312340      0.086335   \n",
      "1576       0        0.908381  0.252745      0.388124      0.329457   \n",
      "1577       0        1.936325  0.181232      0.252147      0.134907   \n",
      "1578       0        0.488903  0.082515      0.331552      0.161454   \n",
      "\n",
      "      oxygen_rate  iron_rate  aluminium_rate  magnesium_rate  undetectable  \n",
      "0        0.041641   0.334293        0.267760        0.119801      0.034630  \n",
      "1        0.063750   0.292194        0.427858        0.134862      0.008480  \n",
      "2        0.044416   0.467126        0.104709        0.141492      0.047622  \n",
      "3        0.009568   0.649819        0.186275        0.102852      0.008250  \n",
      "4        0.018702   0.538314        0.245560        0.056985      0.074020  \n",
      "...           ...        ...             ...             ...           ...  \n",
      "1574     0.048891   0.181620        0.381612        0.230426      0.086299  \n",
      "1575     0.009280   0.584654        0.141368        0.102890      0.075474  \n",
      "1576     0.112298   0.187824        0.241526        0.027420      0.101476  \n",
      "1577     0.025059   0.687605        0.079573        0.033518      0.039338  \n",
      "1578     0.028738   0.663847        0.074018        0.054996      0.016946  \n",
      "\n",
      "[1579 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# TODO: create pandas dataframe\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the data frame you can access all data for a key with for example:\n",
    "print(\"All data for a feature \\n\", df[\"copium\"])\n",
    "print()\n",
    "\n",
    "# You can access a single sample with:\n",
    "print(\"Single sample from index \\n\", df.iloc[1])\n",
    "print()\n",
    "\n",
    "# You can access all freatures but one with:\n",
    "all_features_without_copium = df.drop(columns='copium')\n",
    "print(\"All features without copium \\n\", all_features_without_copium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Part 1: Analyse data balance\n",
    "\n",
    "The occurance can be retrevied with .value_counts() from a pandas date frame. Here get the occurance of copium in the samples. Is the dataset balanced?\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251\n",
      "1328\n"
     ]
    }
   ],
   "source": [
    "# TODO: Get number of samples with copium and the number of samples without copium.\n",
    "with_copium = len(df[df['copium'] > 0])\n",
    "no_copium = len(df[df['copium'] == 0])\n",
    "\n",
    "print(with_copium)\n",
    "print(no_copium)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Part 2: Balance data, do this exercise later! \n",
    "\n",
    "We have seen what happens with unbalanced data, now try to balance the data set. You will also need to change in ex c) so that it uses the balanced data. We show how it can be done for downsampling the one that is more common, in a similar way your job is to instead create an upsampled balanced data set. You only need to use the upsampled data set for the rest of the other part 2) exerices. \n",
    "\n",
    "What could be the reason for choosing one of these over the other?\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO balance data det. \n",
    "# step 1: seperate the data into something that contains copium and one that doesn't,\n",
    "# can for example be done with df[df[\"copium\"]==0] etc.\n",
    "df_zero = df[df[\"copium\"]==0]\n",
    "df_one = df[df[\"copium\"]==1]\n",
    "\n",
    "# downsample majority\n",
    "df_zero_downsampled = resample(df_zero,\n",
    "                               n_samples=df_one.shape[0])\n",
    "\n",
    "df_balanced_downsampled = pd.concat([df_one, df_zero_downsampled])\n",
    "\n",
    "\n",
    "# TODO: upsample minority \n",
    "\n",
    "df_balanced_upsampled = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c ) Split data\n",
    "\n",
    "Here we will devide the data into a training set and a test set. Good rule of thumb is to use 80% of the data in the training set and 20 % in the test set. The the two data sets should be randomly sampled (shuffle). This is done with train_test_split() from sklearn, https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html. The syntax looks like \n",
    "\n",
    "train, test = train_test_split(dataframe, test_size=ratio_test_set, shuffle=True)\n",
    "\n",
    "Why is it important that the data is shuffled when it is split, what could happen otherwise?\n",
    "\n",
    "Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: devide the data into train and test set. \n",
    "train, test = train_test_split(df, test_size=0.2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Performance evaluation\n",
    "\n",
    "Here we will define a class that later will be used for evaluation the performance of the classification models. More information about precision and recall can be found at https://en.wikipedia.org/wiki/Precision_and_recall. \n",
    "\n",
    "Explain why the different metics are usefull, why is not always accuarcy enough?\n",
    "\n",
    "Answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification_eval(object):\n",
    "    def __init__(self):\n",
    "        # counters \n",
    "        self.TP = 0 # correctly identified positive \n",
    "        self.FP = 0 # falsely identified positive \n",
    "        self.TN = 0 # correctly identified negative \n",
    "        self.FN = 0 # falsely identified negative \n",
    "    \n",
    "    def update(self, pred, label):\n",
    "        \"\"\"\n",
    "        pred - is the prediction will be either a 1 or 0. \n",
    "        label - is the correct answer, will be either a 1 or 0.\n",
    "        \"\"\"\n",
    "        # TODO: add to one of the counters each time this function is called. \n",
    "        if pred == label:\n",
    "            if pred == 1:\n",
    "                self.TP += 1\n",
    "            else:\n",
    "                self.TN += 1\n",
    "        else:\n",
    "            if pred == 1:\n",
    "                self.FP += 1\n",
    "            else: \n",
    "                self.FN += 1\n",
    "\n",
    "    \n",
    "    def accuracy(self): \n",
    "        # returns the accuracy \n",
    "        if (self.TP + self.TN) == 0:\n",
    "            return 0\n",
    "        # TODO: calculate the accuracy.\n",
    "        accuracy = (self.TP + self.TN)/(self.TP + self.TN + self.FP + self.FN)\n",
    "        return np.round(accuracy, 4)\n",
    "    \n",
    "    def precision(self): # percentage of the estimated positive that actually is positive\n",
    "        if self.TP == 0:\n",
    "            return 0\n",
    "        # TODO: calculate the precision.\n",
    "        precision = (self.TP/(self.TP + self.FP))\n",
    "        return np.round(precision, 4)\n",
    "    \n",
    "    def recall(self): # percentage of correctly identified positive of the total positive\n",
    "        if self.TP == 0:\n",
    "            return 0\n",
    "        # TODO: calculate the recall.\n",
    "        recall = (self.TP/(self.TP + self.FN))\n",
    "        return np.round(recall, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: K- nearest neighbours\n",
    "\n",
    "## a) Normalize\n",
    "\n",
    "Here we will code our K-NN classifier, method 2.1 on page 21 in the book has the psudo code for K-NN. We will start with the data normalization, i.e. we will normalize the input data so that each feature has the same range in terms of max/min values. The min value can be found with data.min(), similarly for the max value. \n",
    "\n",
    "Why is it important that the data is normalized for the K-NN algorithm?\n",
    "\n",
    "Answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    def __init__(self):\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "    \n",
    "    def normalize(self, data):\n",
    "        # normalize the data and return it. \n",
    "        return (data-self.min)/(self.max-self.min)\n",
    "    \n",
    "    def update_normalization(self, data):\n",
    "        # Save the min and max values for each feature. This funciton is only used for the training data.\n",
    "        self.min = data.min()\n",
    "        self.max = data.max()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) K-NN\n",
    "Lets make the K-NN algorithm, fill in the TODO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-27-d7cb364a6ddc>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-27-d7cb364a6ddc>\"\u001b[1;36m, line \u001b[1;32m20\u001b[0m\n\u001b[1;33m    features_norm =\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class KNN(object):\n",
    "    def __init__(self, k):\n",
    "        self.features = None # normalized features from training data \n",
    "        self.labels = None # the corresponding labels (if there is copium)\n",
    "        self.normalize = Normalize() # class instance for normalization\n",
    "        self.k = k # the k value in k-nn algorithm.\n",
    "        \n",
    "    def fit(self, features, labels):\n",
    "        # This is where we save the training data. \n",
    "        # TODO: update the normalize filter, normalize the features (save to self.features)\n",
    "        # and save the labels to self.labels\n",
    "        normalize.update_normalization(features)\n",
    "        \n",
    "        self.features = normalize.normalize(features)\n",
    "        self.labels = labels\n",
    "    \n",
    "    def predict(self, features):\n",
    "        # here we get one sample to make a predicion \n",
    "        # TODO normalize the input features. \n",
    "        features_norm = normalize.normalize(features)\n",
    "        \n",
    "        # TODO, loop through all points in the previously saved features and save \n",
    "        # the labels for the k points with the smallest distance to the normalized \n",
    "        # input features in a list, call this list list_prediction. It could for example be inilized with\n",
    "        # list_prediction = [0]*self.k \n",
    "        list_prediction = [0]*self.k\n",
    "        \n",
    "        \n",
    "        return self.majority_vote(list_prediction)\n",
    "    \n",
    "    def majority_vote(self, pred_list):\n",
    "        # Here is a function that will return the majority vote from a list. \n",
    "        keys = list(Counter(pred_list).keys())\n",
    "        occurance = list(Counter(pred_list).values())\n",
    "        idx = np.argmax(occurance)\n",
    "        return keys[idx]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) part 1: Evaluate the K-NN \n",
    "Evaluate the K-NN and choose a suitable k value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train['copium']\n",
    "train_features = train.drop(columns='copium')\n",
    "\n",
    "y = test['copium']\n",
    "x = test.drop(columns='copium')\n",
    "\n",
    "# TODO, try differenent values of k. \n",
    "knn = KNN(k=5)\n",
    "knn.fit(train_features, train_labels)\n",
    "\n",
    "log = Classification_eval()\n",
    "for i in range(x.shape[0]):\n",
    "    pred = knn.predict(x.iloc[i])\n",
    "    log.update(pred, y.iloc[i])\n",
    "\n",
    "print('Accuarcy', log.accuracy())\n",
    "print('Precision', log.precision())\n",
    "print('Recall', log.recall())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Try some differnet values of k and just looking at these resutlts would the klassifier work well for all k? \n",
    "\n",
    "Answer:\n",
    "\n",
    "| k | Accuracy | Precision | Recall | \n",
    "| --- | --- | --- | --- |\n",
    "| 1 |  | |  |  \n",
    "| 5 |  | |  |   \n",
    "| 20 |  | |  |   \n",
    "| 50 |  |  |  |   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) part 2, do later!\n",
    "Now we have balanced data, try the same k values as in part 1. Have the results changed since ex 4 c) part 1? Would this klassifier work better? \n",
    "\n",
    "Answer:\n",
    "\n",
    "| k | Accuracy | Precision | Recall | \n",
    "| --- | --- | --- | --- |\n",
    "| 1 |  | |  |  \n",
    "| 5 |  |  |  |   \n",
    "| 20 |  |  | |   \n",
    "| 50 |  |  | | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Learn tree based classifier\n",
    "\n",
    "Here we will code our tree based classifier. We will start with coding a function that can find the best (according to gini) spliting point for a given data set and then define a recursive class for the Nodes that will make up our tree. \n",
    "\n",
    "## a) Find split point\n",
    "\n",
    "The first step is to define a function that can find the splitting criteria with the highest gini value. \n",
    "\n",
    "The gini value can be described as:\n",
    "\n",
    "If $\\Gamma$ contains the set of all labeles, then $\\Gamma(x_1 < 1)$ would be all labels that belong to the criteria $x_1 < 1$, more generally we could say $\\Gamma(x_i < c)$, where i is the index of one of the features and c is the criteria. Then we can define:\n",
    "\n",
    "$v_1 = mean(\\Gamma(x_i < c))$\n",
    "\n",
    "$v_2 = mean(\\Gamma(x_i \\geq c))$\n",
    "\n",
    "$s_1 = v_1^2 + (1-v_1)^2$\n",
    "\n",
    "$s_2 = v_2^2 + (1-v_2)^2$\n",
    "\n",
    "We define len(x) to give the number of elements of x, then the weighted gini value is:\n",
    "\n",
    "$s = \\frac{len(\\Gamma(x_i < c))}{len(\\Gamma)}*s_1 + \\frac{len(\\Gamma(x_i \\geq c))}{len(\\Gamma)}*s_2$\n",
    "\n",
    "The goal is to split the data so we maximizes $s$, there will be one $s$ for every combination of $x_i$ and $c$. Here we will use a c value that is the average between two data points that are sorted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_split_point(data, label, parameter):\n",
    "    \"\"\"\n",
    "    data - all the data we want to split, our (gamma)\n",
    "    label - the parameter we want to classify. \n",
    "    parameter - the parameter we want to check for, our x_i\n",
    "    -----------\n",
    "    retrun:\n",
    "    split_value - the spliting value, our c. \n",
    "    gini_value - the gini value for the best c.\n",
    "    df_head - the data frame belonging to x_i < c\n",
    "    df_tail - the data frame belonging to x_i => c\n",
    "    \"\"\"\n",
    "    # beging by sorting the data after the paramter. \n",
    "    sorted_data = data.sort_values(by=parameter)\n",
    "    sorted_label = sorted_data[label]\n",
    "    \n",
    "    # TODO loop through all the split points in the sorted data and find \n",
    "    # the best gini_value (s) and split_value (c).  \n",
    "\n",
    "    gini_value =         \n",
    "    split_value =     \n",
    "    # TODO: get the two data frames the corresponds to the split data. \n",
    "    # the functions .head(split_index) and .tail(split_index) could be useful. \n",
    "    df_head = \n",
    "    df_tail = \n",
    "    \n",
    "    return split_value, gini_value, df_head, df_tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Tree Node\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode():\n",
    "    def __init__(self, classification=None):\n",
    "        self.split_value = None # the splitting value (c)\n",
    "        self.split_parameter = None # what feature where uesd for the split (x_i)\n",
    "        self.child_nodes = [] # list that contains two child nodes, if not leaf_node\n",
    "        self.leaf_node = 0 # is this leaf_node (0= no, 1=yes)\n",
    "        self.classification = classification # classification made in this node.\n",
    "        \n",
    "    def predict(self, data):\n",
    "        # TODO: we need to traverse the tree recursivly down to a leaf node.\n",
    "        # step 1: check if this is a leaf node, if it is then return classification otherwise contine with step 2.\n",
    "        # step 2: check the input data for the splitting criteria, i.e. data[x_i] < c ...\n",
    "        # (data[x_i] < c would corresponds to child_node[0] and data[x_i] => c to child_node[1])\n",
    "        # step 3: call the predict function in the corresponding child_node and return the prediction. \n",
    "        \n",
    "            \n",
    "    def learn(self, data, label, min_node_size):\n",
    "        \"\"\"\n",
    "        data - the training data\n",
    "        label - the parameter we want to classify\n",
    "        min_node_size - number of data points in a node for it to become a leaf node. \n",
    "        \"\"\" \n",
    "        # TODO: wirte the learning function. \n",
    "        # Step 1: check if the data fullfils the min_node_size criteria, if so make this node a leaf node and return.\n",
    "        # Step 1.5: Check if the data is homogenious i.e. only contains one type for the labels, if thats \n",
    "        # the case then make this node a leaf node and return.\n",
    "       \n",
    "        # Step 2: Loop over all features and get the best gini and split_value for each feature. \n",
    "        # Step 2.5: Save the best split value and split_paramter and the two new data frames \n",
    "        # corresponding to the split [df_head, df_tail] (these will be data frames for the child nodes).  \n",
    "        \n",
    "        # Step 3: Calculate the majority vote classification of the two data frames. \n",
    "        # Step 3.5: Create two child nodes, (eg child_0 = TreeNode(classification=1)) and and call the \n",
    "        # learn function with the corresponding data frame. \n",
    "        \n",
    "        # Step 4: append the the child node to the self.child_nodes. It should be in the order \n",
    "        # of the child node correspoinding to [df_head, df_tail].\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Train the Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = TreeNode() # create root node\n",
    "# learn the tree structure\n",
    "tree.learn(train, \"copium\", min_node_size=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = test['copium']\n",
    "x = test.drop(columns='copium')\n",
    "log = Classification_eval()\n",
    "\n",
    "for i in range(x.shape[0]):\n",
    "    pred = tree.predict(x.iloc[i])\n",
    "    log.update(pred, y.iloc[i])\n",
    "        \n",
    "print('accuarcy', log.accuracy())\n",
    "print('precision', log.precision())\n",
    "print('recall', log.recall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) part 1\n",
    "\n",
    "Try some differnet values of min_node_size. How does these differ from the K-NN?\n",
    "\n",
    "Answer:\n",
    "\n",
    "| min_node_size | Accuracy | Precision | Recall | \n",
    "| --- | --- | --- | --- |\n",
    "| 1  |  |  |  |  \n",
    "| 10 |  |  |  |   \n",
    "| 20 |  |  |  |   \n",
    "| 50 |  |  |  |   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) part 2, Try with balanced data, do this later!\n",
    "\n",
    "Try some differnet values of min_node_size.\n",
    "\n",
    "Answer:\n",
    "\n",
    "| min_node_size | Accuracy | Precision | Recall | \n",
    "| --- | --- | --- | --- |\n",
    "| 1  |  | |  |  \n",
    "| 10 |  | |  |   \n",
    "| 20 |  | |  |   \n",
    "| 50 |  | |  | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exersice 6: Deployment\n",
    "\n",
    "Here we will try the learned classifiers on a larger map. Make sure that the last run version of K-NN and tree have good parameters i.e. k and min_node_size values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(map_type=2, fps=5, resolution=(1000, 1000))\n",
    "\n",
    "sensor_properties = env.get_sensor_properties()\n",
    "sensor_sample = dict()\n",
    "for key in sensor_properties:\n",
    "    sensor_sample[key] = [0]\n",
    "\n",
    "log_knn = Classification_eval()\n",
    "log_tree = Classification_eval()\n",
    "\n",
    "    \n",
    "for i in range(500):\n",
    "    action = breadth_first_search(actor=env.get_actor(), max_depth=3, action_space=env.get_action_space())\n",
    "    env.step(action)\n",
    "    if env.get_sensor_readings() is not None:\n",
    "        sensor_readings = env.get_sensor_readings()\n",
    "        for key in sensor_readings:\n",
    "            sensor_sample[key][0] = sensor_readings[key]\n",
    "        sensor_sample_df = pd.DataFrame(sensor_sample)\n",
    "        log_knn.update(knn.predict(sensor_sample_df.iloc[0]), env.get_ground_truth())\n",
    "        log_tree.update(tree.predict(sensor_sample_df.iloc[0]), env.get_ground_truth())\n",
    "        env.plt_acc.update_acc(log_tree.accuracy(), log_knn.accuracy())\n",
    "    env.render()\n",
    "\n",
    "env.exit()\n",
    "\n",
    "print(\"K-NN accuracy \", log_knn.accuracy(), \"Tree accuracy\", log_tree.accuracy())\n",
    "print(\"Number of copium deposits foun, K-NN:\", log_knn.TP, \" Tree:\", log_tree.TP)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exersice 7: Balance data\n",
    "\n",
    "Go to 2 b) part 2 and balance the data, then do part 2 on the exersices that have it. Lastly run execise 6 with the classifers trained on ballanced data. What is the major difference?\n",
    "\n",
    "Answer:\n",
    "\n",
    "| Balanced | Accuracy k-nn| nr found copium k-nn | Accuracy tree | nr found copium tree|  \n",
    "| --- | --- | --- | --- | --- |\n",
    "| NO |  |  |  |  | \n",
    "| YES | |  |  |  | "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
